%
% File yantosca_COSC6336_assg_1.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{alltt}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{
  tick label style={font=\tiny},
  label style={font=\footnotesize},
  legend style={font=\tiny},
  compat=newest,
  empty line=scanline
}
\pgfplotstableset{
  col sep=comma,
}

\pgfplotstableread{./pmicro.csv}\pmicro
\pgfplotstableread{./rmicro.csv}\rmicro
\pgfplotstableread{./amicro.csv}\amicro
\pgfplotstableread{./f1micro.csv}\fmicro
\pgfplotstableread{./pmacro.csv}\pmacro
\pgfplotstableread{./rmacro.csv}\rmacro
\pgfplotstableread{./amacro.csv}\amacro
\pgfplotstableread{./f1macro.csv}\fmacro

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{HMM-HPS: A Heuristic Preseed Approach to Part-of-Speech Tagging with Hidden Markov Models}

\author{Michael Yantosca \\
  Department of Computer Science, University of Houston / Houston, TX \\
  {\tt mike@archivarius.net} \\}

\date{2018-03-04}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Methodology}

\paragraph{Training}
Initial development of the POS tagger began with building a frequency counter for
arbitrary order n-grams encountered over the course of reading the training file.
These n-grams included the $n$ pairs of words and corresponding language identification
tags up to the current word being read. As each new word was encountered, each
order of n-gram leading up to the word was counted. Preceding words and language ID
tags were kept in a bounded cursor to minimize processing overhead. If the cursor
was full when it encountered a new word, the oldest entry was excised so that the
new entry could be introduced and tallied accordingly.

\paragraph{}
Additionally, the count of each annotated POS tag was recorded, along with the count
of the transition from the previously seen POS tag. At the beginning of a sentence,
an implicit \texttt{Q0} tag served as the origin of the transition, and at the end
of a sentence, an implicit textttt{QF} tag served as the terminus of the transition.

\paragraph{}
Once the training file had been read and the frequency model established,
the frequency counts were converted to probabilities. At first, Laplace smoothing
\cite[47]{JurafskyMartin}
with equiprobable weights was used to supply probability mass for out-of-vocabulary
words and unseen state transitions, but this was ultimately generalized to add-$k$
smoothing \cite[49]{JurafskyMartin} to permit tuning based on empirical results.
Other smoothing methods were briefly considered but rejected on account of the time
involved to implement them given the schedule of the competition.

\paragraph{}
No attempt was made to store the model in a form that could be loaded later,
so training was done online in the same execution run as the testing.
Whereas this significantly slowed development velocity at the beginning,
the addition of later refinements and the stream-processing approaches
employed in both training and testing made this an acceptable cost.

\paragraph{}
It should be noted that all training was done with the provided \texttt{train.conll}.
On account of time constraints, more sophisticated methods of cross-validation
were not employed.

\paragraph{Testing}
The testing phase employed the Viterbi decoding algorithm as given in Jurafsky and
Martin \shortcite[131-134]{JurafskyMartin}. To save on computation time and memory usage,
a running $a_{max}$ value was kept in the innermost loop for determining each
state's most likely predecessor.

\paragraph{}
To gauge efficacy of the tagger during development and enable rapid prototyping,
a rough measure of precision was taken via the following shell command:

\begin{alltt}
  \tiny{diff dev.conll <output file> | egrep '<' | wc -l}
\end{alltt}

\paragraph{}
All development testing prior to submission was done with the provided
\texttt{dev.conll}, and this drove decisions on refining and tuning the model.
Submissions to the competition were only trained with \texttt{train.conll}.

\paragraph{Refinements}
After the first pass of the tagger pipeline was completed, it was clear that it would
not be able to compete effectively on its own. Rough estimates of precision from
initial assays against the \texttt{dev.conll} ran around 0.81, which was below the
Naive Bayes submission to the competition by the user with the handle blackbishop.
Incorporating some of the ideas given by Dr. Solorio during lecture on February, 28, 2018,
a heuristic fallback step was introduced in the decoding phase for unknown vocabulary.

\paragraph{}
The heuristic fallback employed regular expressions to capture whether the unigram
in the emission was capitalized, contained only punctuation marks, or only alphanumeric
characters. In the capitalization case, the fallback emission probability was zeroed out
for every class except \texttt{PROPN}. In the case where only punctuation marks were
present, the fallback emission probability was zeroed out for every class except
\texttt{PUNCT}. In the case where only alphanumeric characters were present, the fallback
emission probabilities for $P(word|\texttt{PUNCT})$ and $P(word|\texttt{SYM})$ were
zeroed out.

\paragraph{}
This simple heuristic check was done as it was noticed that many clearly nominal or
verbal tokens were being labeled as \texttt{PUNCT}, and it performed as expected,
raising the correctness of the output, if not perfectly preserving the probability masses
involved. Adding this refinement also incurred a massive performance penalty since
the series of branching if-statements became quadratic with respect to $Q$, the set
of possible generating POS states.

\paragraph{}
After the addition of heuristic fallback, attention was turned to combining the
emission probabilities of different orders of n-grams. Two modes of operation were
implemented to achieve this: backoff and interpolation. The backoff mode simply backed off
to increasingly lower orders of n-grams from the highest order stored by the model until
it reached the unigram order. If the model had not seen the token, it would fall back
to a special unknown token encoded as \texttt{<UNK>} that had been seeded with smoothed
probabilities if one of the heuristic fallback cases did not apply.

\paragraph{}
The interpolation mode was implemented in a rudimentary fashion. The highest order n-gram
would receive a weight of 0.5, the (n-1)-gram would receive half of the remaining weight, and so on until the unigram observation. If any of the n-grams had not been seen, its
contribution would fall back to the heuristic fallback unigram of \texttt{<UNK>}.
At the beginnings of sentences and in sentences shorter than the highest order n-gram
in the model, no phantom n-grams were computed. The interpolation chain would start with
the longest observed n-gram and apply the formula in the same manner.

\paragraph{}
An issue was encountered in the dev set with a sentence composed of 91 tokens. A numerical
underflow in the multiplication of probabilities during the decoding phase led to a state
where the backpointers could not be traversed since they would only be assigned if a
preceding state had greater than zero probability, the initial value of the $a_{max}$
cursor. To counter this, a major refactoring of the code was done to store and use
the logarithm of the various probabilities calculated and multiplication operations
were changed to addition. To provide the same effect in log space, the value zero
was replaced with $-\texttt{sys.float\_info.max}$.

\paragraph{}
Since the abysmal execution speed  of the heuristic fallback put a drag on development time,
the heuristic intuitions were integrated earlier in the pipeline by preseeding the
emission probabilities that corresponded to the various heuristic checks. Three
heuristic check responses were encoded in each observation key, and the regular
expression match that provided the key was calculated once upon ingestion during
the decoding phase. The three features that were added to the word and language ID
as part of the observation were as follows:

\itemize
\item{\texttt{SOME\_WORD}: at least one alphanumeric character}
\item{\texttt{CAP}: capitalized alphanumeric followed by alphanumeric}
\item{\texttt{ALL\_PUNCT}: only punctuation characters }

\paragraph{}
This provided a significant performance boost that enabled development to progress
more swiftly while maintaining the same level of precision. Some attempts were made to
redistribute the probability mass appropriately, but in most cases heuristic downgrades
and upgrades during the conversion of frequencies to probabilities were done unilaterally
for the sake of time.

\paragraph{}
Furthermore, the iterative decay of significance in the weights for the rough
interpolation of multiple orders of n-grams was modified to give more credence
to higher-order n-grams. Instead of $\frac{1}{2}$ of the remaining weight, each n-gram
in the chain would receive $\frac{3}{4}$ of the remaining weight.

\paragraph{}
As a final step, emission probabilities for POS states \texttt{INTJ}, \texttt{UNK},
and \texttt{X} were downgraded for unknown words that matched the \texttt{SOME\_WORD}
feature. This was done as a result of observations that these tags seemed to be
disproportionately assigned in pre-submission test runs on \texttt{dev.conll}.
The decision was made in the full understanding of the risk of overfitting on the
admittedly unfounded presumption that unknown ``dictionary'' words would be far more
likely to turn up in a corpus than novel interjections.

\section{Experimental Results}

\paragraph{Competition Submissions}
Results from submissions to the competition from training on \texttt{train.conll}
and testing on \texttt{test.conll} were as follows:

\paragraph{}
{\small
\begin{tabular}{llll}
  Accuracy & $n$ & $k$ & Heuristic \\
  0.9181 & 1 & 1 & fallback\footnote{Regular probabilities.} \\
  0.9352 & 1 & 0.01 & fallback\footnote{Log-space probabilities hereafter.} \\
  0.9347 & 2 & 0.05 & preseed \\
  0.9349 & 1 & 0.01 & preseed \\
  0.9355 & 4 & 0.01 & preseed, revised weights
\end{tabular}}

\paragraph{Local Experimentation}
While the competition submissions exhibited decent overall accuracy,
a more thorough examination of the model's performance in
terms of accuracy, precision, recall, and F1 measure controlling
for certain parameters would provide a more incisive inquiry
into the strengths and weaknesses of this hybrid heuristic HMM approach.
In the interest of ensuring clarity in the derivation of these measures,
the following values are defined:

{\tiny
\begin{align*}{}
  WC =& \texttt{\$(wc -l dataset/dev.conll)} \\
  GP =& \texttt{\$(egrep '\emph{class}\$' dataset/dev.conll | wc -l)} \\
  GF =& WC - GP \\
  FN =& \texttt{\$(diff dataset/dev.conll ../hw1-results/dev4.txt | } \\
     &\  \texttt{egrep '<' | egrep '\emph{class}\$' | wc -l)} \\
  FP =& \texttt{\$(diff dataset/dev.conll ../hw1-results/dev4.txt | } \\
     &\  \texttt{egrep '>' | egrep '\emph{class}\$' | wc -l)} \\
  TP =& GP - FP \\
  TN =& GN - FN
\end{align*}}

\paragraph{}
problems encountered:
- bugs
- failure to reproduce results with the same exec params
-- first of equiprobable come, first serve
- float\_info.min vs. -float\_info.max (relevant when swapping to log prob)

with heuristic fallback vs. without

add-k smoothing (varying k)

backoff vs. rudimentary interpolation

varying n-gram order

\section{Conclusions}

training set too sparse in higher-order n-grams to be of much use
most accurate in 1-grams controlling for other variables until adjusted n-gram weights, at which point higher-order n-grams provided better accuracy (up to 4, then decay?)
decreasing k increased accuracy for dev and test sets, but may not apply in cases with many new unknown vocab
better interpolation, proper selection of lambda factors
better smoothing (i.e., more intelligent distribution of prob mass over unknowns)
lookahead: function of multi-class words often dependent on following context, not only preceding
future work: transfer of language ID to state instead of feature

\section*{Acknowledgments}

The author wishes to acknowledge Dr. Thamar Solorio for heuristic feature suggestions in
lecture on February 28, 2018; the NAACL template authors for providing a ready-made
{\LaTeX} template; and Daniel Jurafsky and James H. Martin for succinctly laying out the
properties and implementation of hidden Markov Models in the 3rd edition draft of their
text \emph{Speech and Language Processing}.

\bibliography{yantosca_COSC6336_assg_1}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\end{document}
