%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}
\usepackage{alltt}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{HMM-HPS: A Heuristic Preseed Approach to Part-of-Speech Tagging with Hidden Markov Models}

\author{Michael Yantosca \\
  Department of Computer Science, University of Houston / Houston, TX \\
  {\tt mike@archivarius.net} \\}

\date{2018-03-04}

\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Methodology}

\paragraph{Training}
Initial development of the POS tagger began with building a frequency counter for
arbitrary order n-grams encountered over the course of reading the training file.
These n-grams included the $n$ pairs of words and corresponding language identification
tags up to the current word being read. As each new word was encountered, each
order of n-gram leading up to the word was counted. Preceding words and language ID
tags were kept in a bounded cursor to minimize processing overhead. If the cursor
was full when it encountered a new word, the oldest entry was excised so that the
new entry could be introduced and tallied accordingly.
\paragraph{}
Once the training file had been read and the frequency model established,
the frequency counts were converted to probabilities. At first, Laplace smoothing
\cite[47]{JurafskyMartin}
with equiprobable weights was used to supply probability mass for out-of-vocabulary
words and unseen state transitions, but this was ultimately generalized to add-$k$
smoothing \cite[49]{JurafskyMartin} to permit tuning based on empirical results.
Other smoothing methods were briefly considered but rejected on account of the time
involved to implement them given the schedule of the competition.

\paragraph{}
No attempt was made to store the model in a form that could be loaded later,
so training was done online in the same execution run as the testing.
Whereas this significantly slowed development velocity at the beginning,
the addition of later refinements and the stream-processing approaches
employed in both training and testing made this an acceptable cost.

\paragraph{Testing}
The testing phase employed the Viterbi decoding algorithm as given in Jurafsky and
Martin \shortcite[131-134]{JurafskyMartin}. To save on computation time and memory usage,
a running $a_{max}$ value was kept in the innermost loop for determining each
state's most likely predecessor.

\paragraph{Refinements}
Heuristic fallback
Rudimentary interpolation, n-gram backoff
Iterative refinement initially testing dev -> dev, then train -> dev
moved to log probabilities
Heuristic preseed
integration of heuristic intuitions earlier in pipeline afforded performance boost in decoding by not having to do heuristic checks all the time (wider feature set)/caveat: probability mass not redistributed in all cases
N-gram weight adjustment
Five submissions to the competition

rapid development prototyping - back of the envelope accuracy measure with diff <1> <2> | egrep '<' | wc -l

\section{Experimental Results}

\begin{alltt}
  $N$ = \$(wc -l dataset/dev.conll)
  $GP$ = \$(egrep '\emph{class}\$' dataset/dev.conll | wc -l)
  $GF$ = $N$ - $GF$
  $FN$ = \$(diff dataset/dev.conll ../hw1-results/dev4.txt | egrep '<' | egrep '\emph{class}\$' | wc -l)
  $FP$ = \$(diff dataset/dev.conll ../hw1-results/dev4.txt | egrep '>' | egrep '\emph{class}\$' | wc -l)
  $TP = GP - FP$
  $TN = GN - FN$
\end{alltt}

problems encountered:
- bugs
- failure to reproduce results with the same exec params
-- first of equiprobable come, first serve
- numerical underflow
- float\_info.min vs. -float\_info.max (relevant when swapping to log prob)

with heuristic fallback vs. without

add-k smoothing (varying k)

backoff vs. rudimentary interpolation

varying n-gram order

\section{Conclusions}

training set too sparse in higher-order n-grams to be of much use
most accurate in 1-grams controlling for other variables until adjusted n-gram weights, at which point higher-order n-grams provided better accuracy (up to 4, then decay?)
decreasing k increased accuracy for dev and test sets, but may not apply in cases with many new unknown vocab
better interpolation, proper selection of lambda factors
better smoothing (i.e., more intelligent distribution of prob mass over unknowns)
lookahead: function of multi-class words often dependent on following context, not only preceding
future work: transfer of language ID to state instead of feature

\section*{Acknowledgments}

The author wishes to acknowledge Dr. Thamar Solorio for heuristic feature suggestions in
lecture on February 28, 2018; the NAACL template authors for providing a ready-made
{\LaTeX} template; and Daniel Jurafsky and James H. Martin for succinctly laying out the
properties and implementation of hidden Markov Models in the 3rd edition draft of their
text \emph{Speech and Language Processing}.

\bibliography{yantosca_COSC6336_assg_1}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\end{document}
